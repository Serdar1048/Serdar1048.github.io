[
    {
        "id": 1,
        "title": "Fake Social Media Account Detection",
        "description": "Digital Detective: Hunting Fakes in the Feed We built a machine that knows the difference between a person and a script.",
        "image": "assets/images/1-1768326691.jpg",
        "github": "https://github.com/Serdar1048/fake-social-media-account-detection",
        "demo_url": "https://fake-social-media-account-detection.streamlit.app/",
        "technologies": [
            "Python",
            "Sklearn",
            "Pandas",
            "Matplotlib",
            "Joblib"
        ],
        "details": "# ðŸ•µï¸â€â™‚ï¸ Sosyal Medya Anomali Tespiti: Teknik Derinlik Raporu\n\n**Tarih:** 13 Ocak 2026  \n**HazÄ±rlayan:** Serdar DedebaÅŸ  \n\n---\n\n## 1. GiriÅŸ: Dijital Maskelerin ArdÄ±ndaki Ä°statistik\n\nSosyal medya platformlarÄ±, milyarlarca kullanÄ±cÄ±nÄ±n etkileÅŸime girdiÄŸi devasa veri okyanuslarÄ±dÄ±r. Ancak bu okyanusta, organik kullanÄ±cÄ±larÄ±n yanÄ± sÄ±ra, manipÃ¼lasyon amacÄ±yla Ã¼retilmiÅŸ sentetik kimlikler (fake/bot hesaplar) de yÃ¼zmektedir. Bu projenin temel amacÄ±, bir sosyal medya hesabÄ±nÄ±n \"insani\" mi yoksa \"yazÄ±lÄ±msal\" mÄ± olduÄŸunu istatistiksel izler Ã¼zerinden tespit etmektir.\n\nBu rapor, geliÅŸtirilen yapay zeka modelinin teknik mimarisini, veri Ã¼zerindeki keÅŸiflerimizi, Ã¶zellik mÃ¼hendisliÄŸi kararlarÄ±mÄ±zÄ± ve modelin karar mekanizmasÄ±nÄ± detaylÄ± bir ÅŸekilde belgelemektedir. AmacÄ±mÄ±z sadece \"bu hesap sahte\" demek deÄŸil, **\"neden sahte olduÄŸunu\"** matematiksel kanÄ±tlarla sunmaktÄ±r.\n\n---\n\n## 2. Veri KeÅŸfi ve Analizi (Exploratory Data Analysis)\n\nHer baÅŸarÄ±lÄ± makine Ã¶ÄŸrenmesi projesi, veriyi anlamakla baÅŸlar.\n\n### 2.1. Veri KaynaÄŸÄ± ve Ã–zellik Havuzu ðŸ—‚ï¸\nProjede kullanÄ±lan veri seti, gerÃ§ek dÃ¼nya senaryolarÄ±nÄ± yansÄ±tmak amacÄ±yla **[Kaggle: Fake/Authentic User Instagram](https://www.kaggle.com/datasets/krpurba/fakeauthentic-user-instagram)** kaynaÄŸÄ±ndan temin edilmiÅŸtir. Veri seti, kullanÄ±cÄ±larÄ±n profil bilgilerinden iÃ§erik paylaÅŸÄ±m alÄ±ÅŸkanlÄ±klarÄ±na kadar geniÅŸ bir yelpazede 18 farklÄ± Ã¶zellik iÃ§ermektedir.\n\nBu Ã¶zellikler, bir hesabÄ±n karakteristiÄŸini ortaya koyan dijital parmak izleri gibidir:\n\n**A. Hesap Profili Ã–zellikleri**\n*   **`pos` (Post SayÄ±sÄ±):** KullanÄ±cÄ±nÄ±n toplam paylaÅŸtÄ±ÄŸÄ± gÃ¶nderi sayÄ±sÄ±. Botlar genelde ya Ã§ok az (yeni aÃ§Ä±lmÄ±ÅŸ) ya da Ã§ok fazla (spam) gÃ¶nderiye sahiptir.\n*   **`flg` (Takip Edilen):** Takip ettiÄŸi kiÅŸi sayÄ±sÄ±. Botlar genelde takipÃ§i kazanmak iÃ§in \"Takibe Takip\" (f4f) stratejisi izler.\n*   **`flr` (TakipÃ§i):** HesabÄ± takip eden kiÅŸi sayÄ±sÄ±.\n*   **`bl` (Biyografi UzunluÄŸu):** Profil aÃ§Ä±klamasÄ±ndaki karakter sayÄ±sÄ±.\n*   **`pic` (Profil Resmi):** Profil resmi var mÄ± (1) yok mu (0). GerÃ§ek kullanÄ±cÄ±larÄ±n Ã§oÄŸunda resim olurken, botlar bazen varsayÄ±lan avatarda kalÄ±r.\n*   **`lin` (DÄ±ÅŸ BaÄŸlantÄ±):** Biyografide link olup olmamasÄ±. Spam hesaplar genelde reklam veya zararlÄ± linkler barÄ±ndÄ±rÄ±r.\n\n**B. Ä°Ã§erik ve PaylaÅŸÄ±m Ã–zellikleri**\n*   **`cl` (AÃ§Ä±klama UzunluÄŸu):** GÃ¶nderi altÄ±ndaki metinlerin ortalama karakter uzunluÄŸu.\n*   **`cz` (BoÅŸ AÃ§Ä±klama OranÄ±):** Ã‡ok kÄ±sa (3 karakterden az) aÃ§Ä±klamalÄ± gÃ¶nderilerin oranÄ±. Botlar genelde aÃ§Ä±klama yazmaz.\n*   **`ni` (Resim DÄ±ÅŸÄ± Medya):** Video veya kaydÄ±rmalÄ± (carousel) postlarÄ±n oranÄ±. Botlar genelde basit resimler yÃ¼kler.\n*   **`lt` (Konum Etiketi):** GÃ¶nderilerde konum (lokasyon) kullanma oranÄ±.\n*   **`hc` (Hashtag SayÄ±sÄ±):** GÃ¶nderi baÅŸÄ±na ortalama etiket sayÄ±sÄ±.\n*   **`cs` (KosinÃ¼s BenzerliÄŸi):** GÃ¶nderilerin birbirine ne kadar benzediÄŸi. Botlar genelde aynÄ± aÃ§Ä±klamayÄ± veya resmi tekrar tekrar paylaÅŸÄ±r (YÃ¼ksek skor = ÅžÃ¼pheli).\n*   **`pi` (PaylaÅŸÄ±m AralÄ±ÄŸÄ±):** Ä°ki post arasÄ±ndaki ortalama sÃ¼re (saat). Ä°mkansÄ±z sÄ±kÄ±lÄ±kta ve dÃ¼zenlilikte paylaÅŸÄ±mlar bot belirtisidir.\n\n**C. EtkileÅŸim ve Anahtar Kelime Ã–zellikleri**\n*   **`erl` / `erc` (EtkileÅŸim OranlarÄ±):** BeÄŸeni ve yorum sayÄ±sÄ±nÄ±n takipÃ§i sayÄ±sÄ±na oranÄ±. BotlarÄ±n takipÃ§isi Ã§ok ama etkileÅŸimi (like/yorum) genelde Ã§ok dÃ¼ÅŸÃ¼ktÃ¼r.\n*   **`pr` (Promosyon Kelimeleri):** \"Ã‡ekiliÅŸ\", \"repost\", \"yarÄ±ÅŸma\" gibi reklam iÃ§erikli kelimelerin kullanÄ±m sÄ±klÄ±ÄŸÄ±.\n*   **`fo` (TakipÃ§i AvcÄ± Kelimeleri):** \"Takip et\", \"f4f\", \"beÄŸen\" gibi takipÃ§i kasmaya yÃ¶nelik etiketlerin kullanÄ±m sÄ±klÄ±ÄŸÄ±.\n*   **`class` (SÄ±nÄ±f):** Hedef deÄŸiÅŸkenimiz. `f` (fake/sahte) veya `r` (real/gerÃ§ek).\n\n### 2.2. SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± ve Denge\nModelin Ã¶nyargÄ±lÄ± olmamasÄ± iÃ§in veri setinin dengeli olmasÄ± kritiktir. YaptÄ±ÄŸÄ±mÄ±z analizde, \"GerÃ§ek\" (Real) ve \"Sahte\" (Fake) hesaplarÄ±n birbirine yakÄ±n oranlarda daÄŸÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼k. Bu, `Accuracy` (DoÄŸruluk) metriÄŸinin gÃ¼venilir bir performans gÃ¶stergesi olabileceÄŸini iÅŸaret eder.\n\n![Class Distribution](assets/images/md-1-det-1768327811587.png)\n*(Grafik 1: Veri setindeki GerÃ§ek ve Sahte hesaplarÄ±n sayÄ±sal daÄŸÄ±lÄ±mÄ±. Dengeli bir yapÄ±, modelin her iki sÄ±nÄ±fÄ± da eÅŸit aÄŸÄ±rlÄ±kta Ã¶ÄŸrenmesini saÄŸlar.)*\n\n### 2.3. Ã–zelliklerin Korelasyon Matrisi\nHangi Ã¶zelliklerin birbiriyle iliÅŸkili olduÄŸunu (Multicollinearity) ve hangilerinin hedef deÄŸiÅŸkenle (`class`) gÃ¼Ã§lÃ¼ baÄŸ kurduÄŸunu anlamak iÃ§in korelasyon matrisini inceledik.\n\n*   **GÃ¶zlem:** BazÄ± Ã¶zellikler (Ã¶rneÄŸin takipÃ§i sayÄ±sÄ± ve beÄŸeni ortalamasÄ±) arasÄ±nda beklenen pozitif korelasyonlar vardÄ±r. Ancak bizim iÃ§in asÄ±l Ã¶nemli olan, `class` (hedef) deÄŸiÅŸkeni ile olan iliÅŸkilerdir.\n*   **Yorum:** Koyu renkli alanlar, gÃ¼Ã§lÃ¼ iliÅŸkileri temsil eder. Ã–zellikle tÃ¼retilmiÅŸ oranlarÄ±n (Ã¶rneÄŸin takipÃ§i/takip oranÄ±) hedef deÄŸiÅŸkenle iliÅŸkisi dikkat Ã§ekicidir.\n\n![Correlation Heatmap](assets/images/md-1-det-1768327811592.png)\n*(Grafik 2: Ã–zellikler arasÄ± korelasyon haritasÄ±. Renkler iliÅŸkinin yÃ¶nÃ¼nÃ¼ ve ÅŸiddetini gÃ¶sterir.)*\n\n### 2.4. Derinlemesine BakÄ±ÅŸ: \"TakipÃ§i\" ve \"EtkileÅŸim\"\nSahte hesaplarÄ± ele veren en bÃ¼yÃ¼k aÃ§Ä±k, davranÄ±ÅŸsal tutarsÄ±zlÄ±klardÄ±r.\n\n*   **TakipÃ§i (Followers):** GerÃ§ek hesaplarda takipÃ§i sayÄ±sÄ± genellikle logaritmik bir daÄŸÄ±lÄ±m izler (az sayÄ±da kiÅŸi Ã§ok takipÃ§iye sahiptir). Sahte hesaplarda ise bu daÄŸÄ±lÄ±m daha sentetiktir.\n    ![Followers vs Class](assets/images/md-1-det-1768327811594.png)\n    *(Grafik 3: TakipÃ§i sayÄ±larÄ±nÄ±n sÄ±nÄ±f bazÄ±nda daÄŸÄ±lÄ±mÄ± (Boxplot). AykÄ±rÄ± deÄŸerlerin ve medyan farklarÄ±nÄ±n sÄ±nÄ±flarÄ± nasÄ±l ayÄ±rdÄ±ÄŸÄ±na dikkat edin.)*\n\n*   **EtkileÅŸim (Engagement):** Bir bot hesabÄ± binlerce kiÅŸiyi takip edebilir ama \"gerÃ§ek\" bir etkileÅŸim (yorum/beÄŸeni oranÄ±) yaratmak zordur. AÅŸaÄŸÄ±daki grafik, BeÄŸeni OranÄ± (`erl`) ve Yorum OranÄ± (`erc`) arasÄ±ndaki iliÅŸkiyi gÃ¶sterir. Sahte hesaplar genellikle dÃ¼ÅŸÃ¼k etkileÅŸim kÃ¼melerinde toplanÄ±rken, gerÃ§ek hesaplar daha geniÅŸ bir alana yayÄ±lÄ±r.\n    ![Engagement Scatter](assets/images/md-1-det-1768327811596.png)\n    *(Grafik 4: BeÄŸeni ve Yorum oranlarÄ±nÄ±n saÃ§Ä±lÄ±m grafiÄŸi. KÃ¼meler arasÄ± ayrÄ±ÅŸma, modelin bu iki Ã¶zelliÄŸi kullanarak karar verebileceÄŸini kanÄ±tlar.)*\n\n---\n\n## 3. Ã–zellik MÃ¼hendisliÄŸi (Feature Engineering)\n\nHam veri, bir model iÃ§in her zaman yeterli deÄŸildir. Veriyi modelin \"anlayabileceÄŸi\" bir dile Ã§evirmek gerekir.\n\n### 3.1. TÃ¼retilen ve SeÃ§ilen Ã–zellikler\nModelin baÅŸarÄ±sÄ±nÄ±n %80'i doÄŸru Ã¶zelliklerin seÃ§imine baÄŸlÄ±dÄ±r.\n*   **`ratio` (TakipÃ§i / Takip OranÄ±):** Bu, projenin \"YÄ±ldÄ±z Ã–zelliÄŸi\"dir.\n    *   *MantÄ±k:* Bir bot genellikle takipÃ§i kazanmak iÃ§in \"Takibe Takip\" yapar. Bu da oranÄ±nÄ± 1.0'e Ã§eker. GerÃ§ek bir fenomende ise bu oran 1000'lere Ã§Ä±kabilir.\n*   **`ni` (Non-Image Ratio):** Videolu veya resimsiz iÃ§erik oranÄ±. Bot yazÄ±lÄ±mlarÄ± metin tabanlÄ± etkileÅŸimi sever, gÃ¶rsel iÅŸleme maliyetlidir.\n\n### 3.2. DÄ±ÅŸarÄ±da BÄ±rakÄ±lanlar (Dropped Features) ðŸ—‘ï¸\nModeli laboratuvar ortamÄ±ndan Ã§Ä±karÄ±p gerÃ§ek dÃ¼nyaya uyarlarken, \"Teorik Maksimum DoÄŸruluk\" ile \"Pratik KullanÄ±labilirlik\" arasÄ±nda bir takas (trade-off) yapmak zorunda kaldÄ±k. AÅŸaÄŸÄ±daki Ã¶zellikler, modelin eÄŸitiminde etkili olsalar da, son kullanÄ±cÄ±nÄ±n manuel olarak hesaplamasÄ±nÄ±n imkansÄ±z veya Ã§ok zor olmasÄ± nedeniyle sistemden Ã§Ä±karÄ±lmÄ±ÅŸtÄ±r:\n\n*   **`pi` (Posting Interval - PaylaÅŸÄ±m SÄ±klÄ±ÄŸÄ±):** \"Son 30 gÃ¶nderinin atÄ±lma saatleri arasÄ±ndaki standart sapma\" gibi karmaÅŸÄ±k bir metrik. Bunu bir insanÄ±n hesaplamasÄ± dakikalar sÃ¼rer.\n*   **`erl` & `erc` (Engagement Rate Likes/Comments):** BeÄŸeni ve Yorum oranlarÄ±. Bunlar dinamiktir ve anlÄ±k olarak deÄŸiÅŸir. KullanÄ±cÄ±dan \"Toplam BeÄŸeni / Toplam Post / TakipÃ§i SayÄ±sÄ±\" formÃ¼lÃ¼nÃ¼ hesaplamasÄ±nÄ± istemek UX (KullanÄ±cÄ± Deneyimi) katilidir.\n*   **`pr` (Promotional Keywords):** Ä°Ã§erikte ne kadar \"#giveaway\", \"#repost\" gibi promosyon kelimesi geÃ§tiÄŸi. Bunu tespit etmek iÃ§in kullanÄ±cÄ±nÄ±n NLP (DoÄŸal Dil Ä°ÅŸleme) yapmasÄ± gerekir.\n*   **`fo` (Follower Hunter Keywords):** \"#follow4follow\", \"#like4like\" gibi etiketlerin kullanÄ±m sÄ±klÄ±ÄŸÄ±. Yine manuel tespiti Ã§ok zordur.\n\n**Karar:** Bu Ã¶zellikleri Ã§Ä±karmak modelin doÄŸruluÄŸundan Ã¶nemsiz bir miktar (binde 2-3 civarÄ±) gÃ¶tÃ¼rse de, sistemin herkes tarafÄ±ndan 10 saniye iÃ§inde kullanÄ±labilir olmasÄ±nÄ± saÄŸlamÄ±ÅŸtÄ±r. \"HÄ±zlÄ± ve Ä°yi\" bir model, \"MÃ¼kemmel ama KullanÄ±lamaz\" bir modelden Ã¼stÃ¼ndÃ¼r.\n\n![Feature Importance](assets/images/md-1-det-1768327811597.png)\n*(Grafik 5: Modelin karar verirken en Ã§ok hangi Ã¶zelliÄŸe gÃ¼vendiÄŸini gÃ¶steren Ã¶nem sÄ±ralamasÄ±. Listenin baÅŸÄ±ndaki Ã¶zellikler, sahte hesap avcÄ±larÄ±dÄ±r.)*\n\n---\n\n## 4. Model SeÃ§imi ve Performans KarÅŸÄ±laÅŸtÄ±rmasÄ± ðŸŒ² vs ðŸš€\n\nProje kapsamÄ±nda dÃ¶rt farklÄ± gÃ¼Ã§lÃ¼ algoritma \"Arena\"ya Ã§Ä±karÄ±lmÄ±ÅŸ ve kÄ±yasÄ±ya yarÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸtÄ±r. Ä°ÅŸte modellerin test seti Ã¼zerindeki performans karnesi:\n\n| Model | Accuracy (DoÄŸruluk) | F1-Score | ROC-AUC | Durum |\n| :--- | :--- | :--- | :--- | :--- |\n| **XGBoost** | **0.8249** | **0.8177** | **0.9087** | ðŸ¥‡ (Skor Lideri) |\n| **Random Forest** | 0.8222 | 0.8133 | 0.9038 | ðŸŒ² **(SeÃ§ilen Model)** |\n| **Neural Network (MLP)** | 0.8203 | 0.8093 | 0.9011 | YarÄ±ÅŸmacÄ± |\n| **Logistic Regression** | 0.7830 | 0.7752 | 0.8600 | Baseline (Taban) |\n\n### Neden XGBoost DeÄŸil de Random Forest? ðŸ¤”\nTabloya bakÄ±ldÄ±ÄŸÄ±nda XGBoost'un matematiksel olarak (binde 2.7 farkla) daha Ã¼stÃ¼n olduÄŸu gÃ¶rÃ¼lmektedir. Ancak mÃ¼hendislik kararlarÄ± sadece ham puana gÃ¶re verilmez. Projemizde **Random Forest**'Ä±n ana model olarak seÃ§ilmesinin kritik sebepleri ÅŸunlardÄ±r:\n\n1.  **GÃ¼rÃ¼ltÃ¼ye DirenÃ§ (Robustness):** XGBoost, hatalarÄ± minimize etmek iÃ§in agresif (boosting) bir yÃ¶ntem izler. Sosyal medya verilerinde etiketlerin (gerÃ§ek/sahte) hatalÄ± olma ihtimali yÃ¼ksektir. XGBoost bu hatalÄ± verileri \"ezberleyip\" (overfit) skoru yapay olarak ÅŸiÅŸirebilir. Random Forest ise \"Bagging\" yÃ¶ntemiyle Ã§oÄŸunluÄŸun oyuna baktÄ±ÄŸÄ± iÃ§in bu tip gÃ¼rÃ¼ltÃ¼lere karÅŸÄ± daha direnÃ§li ve gÃ¼venilirdir.\n2.  **Genellenebilirlik:** Binde 2'lik (%0.2) bir fark, gerÃ§ek hayatta ihmal edilebilir bir kazanÃ§tÄ±r. Buna karÅŸÄ±lÄ±k Random Forest'Ä±n yeni ve gÃ¶rÃ¼lmemiÅŸ veri tiplerinde daha stabil Ã§alÄ±ÅŸma eÄŸilimi vardÄ±r.\n3.  **AÃ§Ä±klanabilirlik:** Projemizin amacÄ± \"neden sahte?\" sorusuna cevap vermektedir. Random Forest'Ä±n karar mekanizmasÄ±, aÄŸaÃ§ yapÄ±sÄ± sayesinde daha ÅŸeffaf ve izlenebilirdir.\n\n**Optimizasyon SÃ¼reci (GridSearchCV):**\nSeÃ§ilen Random Forest modeli ham haliyle bÄ±rakÄ±lmamÄ±ÅŸ, 72 farklÄ± parametre kombinasyonu test edilerek evrimleÅŸtirilmiÅŸtir.\n*   **`n_estimators`: 200** (Daha stabil karar iÃ§in aÄŸaÃ§ sayÄ±sÄ± iki katÄ±na Ã§Ä±karÄ±ldÄ±)\n*   **`min_samples_split`: 5** (AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi engellemek iÃ§in filtre)\nModeli varsayÄ±lan ayarlarla bÄ±rakmadÄ±k. `GridSearchCV` tekniÄŸi ile 72 farklÄ± kombinasyonu test ederek en iyi sonucu veren \"Genetik KodlarÄ±\" bulduk. Ä°ÅŸte ÅŸampiyon modelin parametreleri:\n\n*   **`n_estimators` (AÄŸaÃ§ SayÄ±sÄ±): 200** - Sistemde 200 farklÄ± karar aÄŸacÄ± aynÄ± anda Ã§alÄ±ÅŸÄ±r. (Standart 100'dÃ¼r, biz iki katÄ±na Ã§Ä±kardÄ±k).\n*   **`max_depth` (Derinlik): None** - AÄŸaÃ§larÄ±n sÄ±nÄ±rsÄ±z derinleÅŸmesine izin verdik, bÃ¶ylece en ince detayÄ± bile yakalayabildiler.\n*   **`min_samples_split`: 5** - Bir dalÄ±n ikiye ayrÄ±lmasÄ± iÃ§in en az 5 veri noktasÄ± olmasÄ± ÅŸartÄ±nÄ± koÅŸtuk (AÅŸÄ±rÄ± ezberlemeyi Ã¶nlemek iÃ§in).\n*   **`min_samples_leaf`: 2** - Her yaprakta en az 2 hesap bulunmasÄ±nÄ± zorunlu kÄ±ldÄ±k.\n\n---\n\n## 5. Performans DeÄŸerlendirmesi: Karnemiz NasÄ±l? ðŸ“Š\n\nModeli \"Test Seti\" (hiÃ§ gÃ¶rmediÄŸi veriler) Ã¼zerinde zorlu bir sÄ±nava soktuk. SonuÃ§lar, gÃ¼venilir bir sistem inÅŸa ettiÄŸimizi kanÄ±tlÄ±yor:\n\n### 5.1. Metrikler ve AnlamlarÄ±\nSadece \"DoÄŸruluk\" (Accuracy) tek baÅŸÄ±na yeterli deÄŸildir. Ä°ÅŸte modelin detaylÄ± karnesi:\n\n| Metrik | DeÄŸer | AnlamÄ± |\n| :--- | :--- | :--- |\n| **Accuracy (DoÄŸruluk)** | **%83.00** | Model her 100 hesabÄ±n 83 tanesini doÄŸru bildi. |\n| **F1-Score** | **0.83** | \"Hem sahteyi hem gerÃ§eÄŸi dengeli bilme\" puanÄ±. (1.0 Ã¼zerinden). |\n| **Precision (Kesinlik)** | **0.83** | \"Sahte\" dediÄŸimiz hesaplarÄ±n %83'Ã¼ gerÃ§ekten sahteydi. (YanlÄ±ÅŸ alarm az). |\n| **Recall (DuyarlÄ±lÄ±k)** | **0.83** | Piyasadaki tÃ¼m sahte hesaplarÄ±n %83'Ã¼nÃ¼ yakalayabildik. (KaÃ§an balÄ±k az). |\n\n**Yorum:** %83'lÃ¼k doÄŸruluk oranÄ±, sosyal medya gibi gÃ¼rÃ¼ltÃ¼lÃ¼ (hatalÄ± etiketlenmiÅŸ verilerin olduÄŸu) bir ortamda oldukÃ§a baÅŸarÄ±lÄ±dÄ±r. F1 skorunun da 0.83 olmasÄ±, modelin \"Sadece gerÃ§ekleri bulayÄ±m, sahteleri boÅŸvereyim\" gibi bir kurnazlÄ±k yapmadÄ±ÄŸÄ±nÄ±, her iki sÄ±nÄ±fÄ± da eÅŸit baÅŸarÄ±yla tanÄ±dÄ±ÄŸÄ±nÄ± gÃ¶sterir.\n\n### 5.2. Confusion Matrix (Hata Matrisi)\nSadece nerede hata yaptÄ±ÄŸÄ±mÄ±zÄ± gÃ¶rmek iÃ§in matrise bakalÄ±m:\n*   **False Positive (YanlÄ±ÅŸ Alarm):** GerÃ§ek birine \"Sahte\" deme oranÄ±mÄ±z dÃ¼ÅŸÃ¼ktÃ¼r.\n*   **False Negative (KaÃ§an BalÄ±k):** Sahte birini \"GerÃ§ek\" sanma oranÄ±mÄ±z dÃ¼ÅŸÃ¼ktÃ¼r.\n\nAÅŸaÄŸÄ±daki matris, modelin kararlÄ±lÄ±ÄŸÄ±nÄ± gÃ¶rselleÅŸtirir. KÃ¶ÅŸegen Ã¼zerindeki yÃ¼ksek yoÄŸunluk, baÅŸarÄ±nÄ±n kanÄ±tÄ±dÄ±r.\n\n![Confusion Matrix](assets/images/md-1-det-1768327811599.png)\n*(Grafik 6: Test seti Ã¼zerindeki tahminlerin gerÃ§ek deÄŸerlerle karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±.)*\n\n### 5.2. ROC EÄŸrisi ve AUC\nROC eÄŸrisi, modelin ayÄ±rt etme gÃ¼cÃ¼nÃ¼ gÃ¶sterir. EÄŸri ne kadar sol Ã¼st kÃ¶ÅŸeye yakÄ±nsa, model o kadar mÃ¼kemmeldir. Bizim modelimiz, Ã§izginin Ã§ok Ã¼zerinde bir performans sergileyerek rastgele bir tahminciden Ã§ok daha Ã¼stÃ¼n olduÄŸunu kanÄ±tlamÄ±ÅŸtÄ±r.\n\n**AUC (Area Under Curve) = 0.91** deÄŸerinin projemizdeki pratik anlamÄ± ÅŸudur:\n1.  **AyÄ±rt Etme GÃ¼cÃ¼:** Sisteme rastgele bir **gerÃ§ek** ve bir **sahte** hesap verildiÄŸinde, modelin sahte hesabÄ± gerÃ§ek hesaba gÃ¶re daha yÃ¼ksek bir risk skoruyla doÄŸru ayÄ±rt etme olasÄ±lÄ±ÄŸÄ± **%91**'dir.\n2.  **GÃ¼venilirlik:** BaÅŸarÄ± ÅŸansÄ±nÄ±n 0.50 olduÄŸu (yazÄ±-tura) rastgele bir tahminciye kÄ±yasla; **0.91**'lik skor, modelin **\"MÃ¼kemmel\" (Excellent)** kategorisinde performans gÃ¶sterdiÄŸini kanÄ±tlar. Bu, modelin sosyal medya gibi karmaÅŸÄ±k ve gÃ¼rÃ¼ltÃ¼lÃ¼ verilerde bile sahte hesaplarÄ± Ã§ok net bir ÅŸekilde ayrÄ±ÅŸtÄ±rabildiÄŸini gÃ¶sterir.\n\n![ROC Curve](assets/images/md-1-det-1768327811600.png)\n*(Grafik 7: True Positive Rate vs False Positive Rate. EÄŸrinin altÄ±ndaki alan (AUC), 0.91'lik deÄŸeriyle modelin Ã¼stÃ¼n baÅŸarÄ±sÄ±nÄ± Ã¶zetler.)*\n\n---\n\n## 6. Hibrit Karar Mimarisi: Ä°statistik + Uzman KuralÄ±\n\nVeri setimiz 2017 yÄ±lÄ±ndan kalmaydÄ± ve o dÃ¶nemde \"Mavi Tik\" veya \"Ã–ne Ã‡Ä±kanlar\" gibi kavramlar bugÃ¼nkÃ¼ kadar yaygÄ±n/anlamlÄ± deÄŸildi. Model, Mavi Tik'in ne olduÄŸunu **bilmiyor**.\n\nBu yÃ¼zden, saf Makine Ã–ÄŸrenmesi Ã§Ä±ktÄ±sÄ±nÄ± (0 ile 1 arasÄ± bir risk skoru), uzman kurallarÄ±yla (Rule-Based) harmanlayan bir **Hibrit Mimari** tasarladÄ±k.\n\nBu mimari, **\"Ä°leri Beslemeli Risk Hesaplama ve Geri Beslemeli Ceza Sistemi\"** (Feed-Forward Risk Calculation with Feedback Penalty) prensibine dayanÄ±r. Sistem iki aÅŸamalÄ± Ã§alÄ±ÅŸÄ±r:\n\n**1. AÅŸama: Ä°statistiksel Risk Tahmini (The Mathematical Core)**\nÄ°lk adÄ±mda, Random Forest modeli hesabÄ±n sadece sayÄ±sal verilerine (takipÃ§i sayÄ±sÄ±, takip edilen sayÄ±sÄ±, gÃ¶nderi sÄ±klÄ±ÄŸÄ± vb.) bakar. Bu aÅŸamada model, hesabÄ±n \"kim\" olduÄŸuyla ilgilenmez, sadece rakamlarÄ±n oluÅŸturduÄŸu deseni inceler. Model, 0 ile 1 arasÄ±nda ham bir **\"Yapay Zeka Risk Skoru\"** Ã¼retir (Ã–rneÄŸin: 0.95 - YÃ¼ksek OlasÄ±lÄ±kla Sahte).\n\n**2. AÅŸama: BaÄŸlamsal GÃ¼ven Ä°ndirimi (Contextual Reliability Discount)**\nÄ°kinci aÅŸamada, modelin eÄŸitim setinde bulunmayan ancak gerÃ§ek dÃ¼nyada gÃ¼venilirlik sinyali olarak kabul edilen \"sosyal kanÄ±tlar\" devreye girer. Bu sinyaller (Mavi Tik, KaydÄ±rmalÄ± Post, Ã–ne Ã‡Ä±kan Hikayeler), hesaplanan risk skorunu \"cezalandÄ±rÄ±r\", yani aÅŸaÄŸÄ± Ã§eker.\n\nBu iÅŸlem basit bir Ã§Ä±karma iÅŸlemi deÄŸil, **Ã§arpÄ±msal bir sÃ¶nÃ¼mleme** (multiplicative decay) iÅŸlemidir:\n\n$$ FinalRisk = HamRisk \\times (1 - GÃ¼venFaktÃ¶rÃ¼_1) \\times (1 - GÃ¼venFaktÃ¶rÃ¼_2) ... $$\n\n*   **Mavi Tik (Verified Badge):** En gÃ¼Ã§lÃ¼ gÃ¼ven sinyalidir. Risk skorunu doÄŸrudan %15 oranÄ±nda sÃ¶nÃ¼mler.\n*   **KaydÄ±rmalÄ± Post (Carousel):** Tek bir gÃ¶nderide birden fazla medyanÄ±n paylaÅŸÄ±lmasÄ± (saÄŸa kaydÄ±rmalÄ± albÃ¼m), basit bot yazÄ±lÄ±mlarÄ±nÄ±n genellikle yapamadÄ±ÄŸÄ± bir eylemdir. Bu \"insani efor\" gÃ¶stergesi, risk skorunu %15 oranÄ±nda azaltÄ±r.\n*   **Ã–ne Ã‡Ä±kanlar (Highlights):** Hikaye arÅŸivi oluÅŸturmak, bir bot yazÄ±lÄ±mÄ± iÃ§in karmaÅŸÄ±k ve maliyetli bir sÃ¼reÃ§tir. Bu Ã¶zellik, hesabÄ±n arkasÄ±nda gerÃ§ek bir insan olduÄŸunun gÃ¼Ã§lÃ¼ bir gÃ¶stergesidir ve riski %5 oranÄ±nda azaltÄ±r.\n\n**SonuÃ§:**\nModel, bir hesaba matematiksel olarak %90 oranÄ±nda \"Sahte\" dese bile, eÄŸer o hesap \"Mavi Tikli\" ve \"Ã–ne Ã‡Ä±kanlara Sahip\" ise, risk skoru dramatik bir ÅŸekilde dÃ¼ÅŸÃ¼rÃ¼lerek (%90 -> %72 -> %68) gÃ¼venli bÃ¶lgeye (GerÃ§ek Hesap) Ã§ekilir. Bu yaklaÅŸÄ±m, yapay zekanÄ±n katÄ± kurallarÄ±nÄ±, insan sezgisiyle esneterek \"False Positive\" (YanlÄ±ÅŸ Alarm) oranÄ±nÄ± minimize eder.\n\nBu mimari sayesinde:\n1.  **Adaptasyon:** Model yeniden eÄŸitilmeden yeni kurallar (Ã¶rn. yeni bir rozet tÃ¼rÃ¼) sisteme eklenebilir.\n2.  **GÃ¼ven:** Veri setinin eksik kaldÄ±ÄŸÄ± yerlerde \"Ä°nsan ZekasÄ±\" devreye girer.\n\n---\n\n## 7. SonuÃ§ ve Yorum\n\nBu proje, bir \"SÄ±nÄ±flandÄ±rma\" probleminden fazlasÄ±dÄ±r; dijital bir dedektiflik Ã§alÄ±ÅŸmasÄ±dÄ±r.\n\n*   Ã‡Ä±plak gÃ¶zle \"normal\" gÃ¶rÃ¼nen bir hesap, **TakipÃ§i/Takip oranÄ±** ve **Ä°Ã§erik TÃ¼rÃ¼ DengesizliÄŸi** (`ni`) sayesinde model tarafÄ±ndan anÄ±nda yakalanabilmektedir.\n*   **Feature Selection** (Ã–zellik SeÃ§imi) aÅŸamasÄ±nda yaptÄ±ÄŸÄ±mÄ±z elemeler, modelin hem hafif hem de sahada uygulanabilir olmasÄ±nÄ± saÄŸlamÄ±ÅŸtÄ±r.\n*   TasarladÄ±ÄŸÄ±mÄ±z **Hibrit Mimari**, saf yapay zekanÄ±n \"kÃ¶r noktalarÄ±nÄ±\" (baÄŸlam eksikliÄŸi) basit ama etkili kurallarla kapatmÄ±ÅŸtÄ±r.\n\nSonuÃ§ olarak ortaya Ã§Ä±kan sistem; veriye dayalÄ±, istatistiksel, aÃ§Ä±klanabilir ve yÃ¼ksek doÄŸruluklu bir anomali tespit motorudur.\n\n---\n\n",
        "details_en": "# ðŸ•µï¸â€â™‚ï¸ Social Media Anomaly Detection: Technical Depth Report\n\n**Date:** January 13, 2026 \n**Prepared By:** Serdar Dedebas\n\n---\n\n## 1. Introduction: Statistics Behind Digital Masks\n\nSocial media platforms are vast oceans of data where billions of users interact. However, swimming alongside organic users in this ocean are synthetic identities (fake/bot accounts) created for manipulation. The primary goal of this project is to detect whether a social media account is \"human\" or \"software\" based on statistical traces.\n\nThis report documents the technical architecture of the developed AI model, our discoveries on the data, feature engineering decisions, and the model's decision mechanism in detail. Our aim is not just to say \"this account is fake,\" but to present **\"why it is fake\"** with mathematical proofs.\n\n---\n\n## 2. Data Discovery and Analysis (Exploratory Data Analysis)\n\nEvery successful machine learning project begins with understanding the data.\n\n### 2.1. Data Source and Feature Pool ðŸ—‚ï¸\nThe dataset used in the project was obtained from **[Kaggle: Fake/Authentic User Instagram](https://www.kaggle.com/datasets/krpurba/fakeauthentic-user-instagram)** to reflect real-world scenarios. The dataset contains 18 different features ranging from user profile information to content sharing habits.\n\nThese features are like digital fingerprints revealing the characteristics of an account:\n\n**A. Account Profile Features**\n*   **`pos` (Number of Posts):** Total number of posts shared by the user. Bots usually have either very few (newly opened) or too many (spam) posts.\n*   **`flg` (Following):** Number of people the user follows. Bots generally follow many people to gain followers (Follow-for-Follow).\n*   **`flr` (Followers):** Number of people following the account.\n*   **`bl` (Bio Length):** Character count of the profile description.\n*   **`pic` (Profile Picture):** Whether there is a profile picture (1) or not (0). Most real users have a picture, whereas bots sometimes stay with the default avatar.\n*   **`lin` (External Link):** Presence of a link in the bio. Spam accounts often contain ad or malicious links.\n\n**B. Content and Sharing Features**\n*   **`cl` (Caption Length):** Average character length of the text under posts.\n*   **`cz` (Empty Caption Ratio):** Ratio of posts with very short descriptions (less than 3 characters). Bots usually don't write descriptions.\n*   **`ni` (Non-Image Media):** Ratio of video or carousel posts. Bots generally upload simple images.\n*   **`lt` (Location Tag):** Ratio of using location tags in posts.\n*   **`hc` (Hashtag Count):** Average number of tags per post.\n*   **`cs` (Cosine Similarity):** How similar the posts are to each other. Bots usually share the same description or image repeatedly (High score = Suspicious).\n*   **`pi` (Posting Interval):** Average time between two posts (in hours). Impossible tightness and regularity are signs of a bot.\n\n**C. Engagement and Keyword Features**\n*   **`erl` / `erc` (Engagement Rates):** Ratio of likes and comments to the number of followers. Bots have many followers but generally very low interaction (like/comment).\n*   **`pr` (Promotional Keywords):** Frequency of use of ad-content words like \"Giveaway\", \"repost\", \"contest\".\n*   **`fo` (Follower Hunter Keywords):** Frequency of use of tags aimed at gaining followers like \"follow me\", \"f4f\", \"like\".\n*   **`class` (Class):** Our target variable. `f` (fake) or `r` (real).\n\n### 2.2. Class Distribution and Balance\nFor the model not to be biased, it is critical that the dataset is balanced. In our analysis, we saw that \"Real\" and \"Fake\" accounts are distributed in close proportions. This indicates that the `Accuracy` metric can be a reliable performance indicator.\n\n![Class Distribution](assets/images/md-1-det-1768327811602.png)\n*(Chart 1: Numerical distribution of Real and Fake accounts in the dataset. A balanced structure ensures the model learns both classes with equal weight.)*\n\n### 2.3. Correlation Matrix of Features\nWe examined the correlation matrix to understand which features are related to each other (Multicollinearity) and which have a strong connection with the target variable (`class`).\n\n*   **Observation:** There are expected positive correlations between some features (e.g., follower count and average likes). However, what matters for us is the relationships with the `class` (target) variable.\n*   **Interpretation:** Dark areas represent strong relationships. Especially the relationship of derived ratios (e.g., follower/following ratio) with the target variable is noteworthy.\n\n![Correlation Heatmap](assets/images/md-1-det-1768327811606.png)\n*(Chart 2: Correlation map between features. Colors show the direction and intensity of the relationship.)*\n\n### 2.4. Deep Dive: \"Followers\" and \"Engagement\"\nThe biggest vulnerability that reveals fake accounts is behavioral inconsistencies.\n\n*   **Followers:** In real accounts, the follower count usually follows a logarithmic distribution (few people have many followers). In fake accounts, this distribution is more synthetic.\n    ![Followers vs Class](assets/images/md-1-det-1768327811608.png)\n    *(Chart 3: Distribution of follower counts by class (Boxplot). Note how outliers and median differences separate the classes.)*\n\n*   **Engagement:** A bot account might follow thousands of people, but it is hard to create \"real\" engagement (comment/like ratio). The chart below shows the relationship between Like Rate (`erl`) and Comment Rate (`erc`). Fake accounts generally cluster in low engagement areas, while real accounts spread to a wider area.\n    ![Engagement Scatter](assets/images/md-1-det-1768327811609.png)\n    *(Chart 4: Scatter plot of Like and Comment rates. The separation between clusters proves the model can decide using these two features.)*\n\n---\n\n## 3. Feature Engineering\n\nRaw data is not always sufficient for a model. It is necessary to translate the data into a language the model can \"understand\".\n\n### 3.1. Derived and Selected Features\n80% of the model's success depends on the selection of the right features.\n*   **`ratio` (Follower / Following Ratio):** This is the \"Star Feature\" of the project.\n    *   *Logic:* A bot usually does \"Follow for Follow\" to gain followers. This pulls the ratio to 1.0. In a real phenomenon, this ratio can go up to 1000s.\n*   **`ni` (Non-Image Ratio):** Ratio of video or image-less content. Bot software likes text-based interaction; image processing is costly.\n\n### 3.2. Dropped Features ðŸ—‘ï¸\nWhen taking the model out of the lab and adapting it to the real world, we had to make a trade-off between \"Theoretical Maximum Accuracy\" and \"Practical Usability\". The following features, although effective in training the model, were removed from the system because it is impossible or very difficult for the end-user to calculate them manually:\n\n*   **`pi` (Posting Interval):** A complex metric like \"Standard deviation between the posting hours of the last 30 posts\". It takes minutes for a human to calculate this.\n*   **`erl` & `erc` (Engagement Rate Likes/Comments):** Like and Comment rates. These are dynamic and change instantly. Asking the user to calculate the \"Total Likes / Total Posts / Follower Count\" formula is a UX (User Experience) killer.\n*   **`pr` (Promotional Keywords):** How many promotional words like \"#giveaway\", \"#repost\" appear in the content. The user would need to perform NLP (Natural Language Processing) to detect this.\n*   **`fo` (Follower Hunter Keywords):** Frequency of usage of tags like \"#follow4follow\", \"#like4like\". Again, manual detection is very difficult.\n\n**Decision:** Although removing these features took away an insignificant amount (around 0.2-0.3%) from the model's accuracy, it ensured the system is usable by everyone within 10 seconds. A \"Fast and Good\" model is superior to a \"Perfect but Unusable\" model.\n\n![Feature Importance](assets/images/md-1-det-1768327811610.png)\n*(Chart 5: Importance ranking showing which feature the model relies on most when deciding. Features at the top of the list are fake account hunters.)*\n\n---\n\n## 4. Model Selection and Performance Comparison ðŸŒ² vs ðŸš€\n\nWithin the scope of the project, four different powerful algorithms were brought to the \"Arena\" and raced fiercely. Here is the performance report card of the models on the test set:\n\n| Model | Accuracy | F1-Score | ROC-AUC | Status |\n| :--- | :--- | :--- | :--- | :--- |\n| **XGBoost** | **0.8249** | **0.8177** | **0.9087** | ðŸ¥‡ (Score Leader) |\n| **Random Forest** | 0.8222 | 0.8133 | 0.9038 | ðŸŒ² **(Selected Model)** |\n| **Neural Network (MLP)** | 0.8203 | 0.8093 | 0.9011 | Contender |\n| **Logistic Regression** | 0.7830 | 0.7752 | 0.8600 | Baseline |\n\n### Why Random Forest and Not XGBoost? ðŸ¤”\nLooking at the table, it is seen that XGBoost is mathematically superior (by a difference of 0.2%). However, engineering decisions are not made solely based on raw scores. The critical reasons why **Random Forest** was selected as the main model in our project are:\n\n1.  **Robustness:** XGBoost follows an aggressive (boosting) method to minimize errors. In social media data, the probability of labels (real/fake) being incorrect is high. XGBoost might \"memorize\" (overfit) these incorrect data and artificially inflate the score. Random Forest, on the other hand, is more resistant and reliable against such noise because it looks at the majority vote with the \"Bagging\" method.\n2.  **Generalizability:** A difference of 0.2% is a negligible gain in real life. In contrast, Random Forest tends to work more stably on new and unseen data types.\n3.  **Explainability:** The aim of our project is to answer the question \"why is it fake?\". Random Forest's decision mechanism is more transparent and traceable thanks to its tree structure.\n\n**Optimization Process (GridSearchCV):**\nThe selected Random Forest model was not left in its raw state, but evolved by testing 72 different parameter combinations.\n*   **`n_estimators`: 200** (Number of trees doubled for a more stable decision)\n*   **`min_samples_split`: 5** (Filter to prevent overfitting)\nWe didn't leave the model with default settings. We found the \"Genetic Codes\" that gave the best result by testing 72 different combinations with the `GridSearchCV` technique. Here are the champion model's parameters:\n\n*   **`n_estimators` (Number of Trees): 200** - 200 different decision trees work simultaneously in the system. (Standard is 100, we doubled it).\n*   **`max_depth` (Depth): None** - We allowed the trees to deepen unlimitedly, so they could catch even the finest detail.\n*   **`min_samples_split`: 5** - We required at least 5 data points for a branch to split into two (To prevent overfitting).\n*   **`min_samples_leaf`: 2** - We mandated at least 2 accounts in each leaf.\n\n---\n\n## 5. Performance Evaluation: How is Our Report Card? ðŸ“Š\n\nWe put the model through a tough exam on the \"Test Set\" (data it has never seen). The results prove that we have built a reliable system:\n\n### 5.1. Metrics and Meanings\nJust \"Accuracy\" alone is not enough. Here is the detailed report card of the model:\n\n| Metric | Value | Meaning |\n| :--- | :--- | :--- |\n| **Accuracy** | **%83.00** | The model correctly identified 83 out of every 100 accounts. |\n| **F1-Score** | **0.83** | The score of \"knowing both fake and real balanced\". (Out of 1.0). |\n| **Precision** | **0.83** | 83% of the accounts we called \"Fake\" were actually fake. (False alarm is low). |\n| **Recall** | **0.83** | We were able to catch 83% of all fake accounts in the market. (Few missed fish). |\n\n**Comment:** An accuracy rate of 83% is quite successful in a noisy environment (where there is mislabeled data) like social media. The F1 score being 0.83 shows that the model does not try to be cunning like \"Just find the real ones, ignore the fakes\", but recognizes both classes with equal success.\n\n### 5.2. Confusion Matrix\nLet's look at the matrix just to see where we made mistakes:\n*   **False Positive (False Alarm):** Our rate of calling a real person \"Fake\" is low.\n*   **False Negative (Missed Fish):** Our rate of mistaking a fake person for \"Real\" is low.\n\nThe matrix below visualizes the stability of the model. High density on the diagonal is proof of success.\n\n![Confusion Matrix](assets/images/md-1-det-1768327811611.png)\n*(Chart 6: Comparison of predictions on the test set with actual values.)*\n\n### 5.3. ROC Curve and AUC\nThe ROC curve shows the discrimination power of the model. The closer the curve is to the top left corner, the more perfect the model is. Our model proved to be far superior to a random predictor by performing well above the line.\n\n**AUC (Area Under Curve) = 0.91** value has the following practical meaning in our project:\n1.  **Discrimination Power:** When given a random **real** and a **fake** account, the probability of the model correctly distinguishing the fake account with a higher risk score than the real account is **%91**.\n2.  **Reliability:** Compared to a random predictor with a 50% chance of success (coin toss); the score of **0.91** proves that the model performs in the **\"Excellent\"** category. This shows that the model can clearly separate fake accounts even in complex and noisy data like social media.\n\n![ROC Curve](assets/images/md-1-det-1768327811612.png)\n*(Chart 7: True Positive Rate vs False Positive Rate. The area under the curve (AUC) summarizes the model's superior success with a value of 0.91.)*\n\n---\n\n## 6. Hybrid Decision Architecture: Statistics + Expert Rule\n\nOur dataset was from 2017, and concepts like \"Blue Tick\" or \"Highlights\" were not as common/meaningful back then. The model **does not know** what a Blue Tick is.\n\nTherefore, we designed a **Hybrid Architecture** that blends pure Machine Learning output (a risk score between 0 and 1) with expert rules (Rule-Based).\n\nThis architecture is based on the **\"Feed-Forward Risk Calculation with Feedback Penalty\"** principle. The system works in two stages:\n\n**Stage 1: Statistical Risk Prediction (The Mathematical Core)**\nIn the first step, the Random Forest model looks only at the numerical data of the account (follower count, following count, posting frequency, etc.). At this stage, the model does not care \"who\" the account is, it only examines the pattern formed by the numbers. The model produces a raw **\"AI Risk Score\"** between 0 and 1 (Example: 0.95 - High Probability Fake).\n\n**Stage 2: Contextual Reliability Discount**\nIn the second stage, \"social proofs\" that are not in the model's training set but are accepted as reliability signals in the real world come into play. These signals (Blue Tick, Carousel Post, Highlight Stories) \"penalize\", i.e., pull down, the calculated risk score.\n\nThis process is not a simple subtraction, but a **multiplicative decay** process:\n\n$$ FinalRisk = RawRisk \\\\times (1 - TrustFactor_1) \\\\times (1 - TrustFactor_2) ... $$\n\n*   **Verified Badge (Blue Tick):** It is the strongest trust signal. It directly dampens the risk score by 15%.\n*   **Carousel Post:** Sharing multiple media in a single post (album with right swipe) is an action that simple bot software usually cannot do. This \"human effort\" indicator reduces the risk score by 15%.\n*   **Highlights:** Creating a story archive is a complex and costly process for bot software. This feature is a strong indicator that there is a real human behind the account and reduces the risk by 5%.\n\n**Result:**\nEven if the model mathematically says \"Fake\" by 90% for an account, if that account is \"Blue Ticked\" and \"Has Highlights\", the risk score is dramatically reduced (%90 -> %72 -> %68) and pulled to the safe zone (Real Account). This approach minimizes the \"False Positive\" (False Alarm) rate by stretching the strict rules of artificial intelligence with human intuition.\n\nThanks to this architecture:\n1.  **Adaptation:** New rules (e.g., a new type of badge) can be added to the system without retraining the model.\n2.  **Trust:** \"Human Intelligence\" steps in where the dataset falls short.\n\n---\n\n## 7. Conclusion and Comment\n\nThis project is more than a \"Classification\" problem; it is a digital detective work.\n\n*   An account that looks \"normal\" to the naked eye can be instantly caught by the model thanks to the **Follower/Following ratio** and **Content Type Imbalance** (`ni`).\n*   The eliminations we made during the **Feature Selection** phase ensured that the model is both light and applicable in the field.\n*   The **Hybrid Architecture** we designed covered the \"blind spots\" of pure artificial intelligence (lack of context) with simple but effective rules.\n\nAs a result, the resulting system is a data-driven, statistical, explainable, and high-accuracy anomaly detection engine.\n\n---\n\n"
    }
]